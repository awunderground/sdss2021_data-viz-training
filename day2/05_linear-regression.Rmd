---
title: "Data Viz and Regression in R"
author: "Aaron R. Williams"
output:
  html_document:
    code_folding: show
    toc: TRUE
    toc_float: TRUE
    css: !expr here::here("www", "web_report.css")
    editor_options:
      chunk_output_type: console
---
<style>
@import url('https://fonts.googleapis.com/css?family=Lato&display=swap');
</style>

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato" />

```{css echo=FALSE}
h1 {
    font-size: 34px;
    color: #337AB7;
}
p {
    margin: 20px 0 20px;
}
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(modelr)
library(broom)
library(urbnthemes)

set_urbn_defaults(style = "print")

options(scipen = 999)
```

This guide is a brief introduction to code for implementing and visually evaluating linear regression models in R. It does not address the theory behind linear regression models.  

# `lm()`

Linear regression models like OLS regression are estimated in R with `lm()`. Here is a simple OLS model with stopping distance as the dependent variable and speed as the independent variable. 

```{r}
lm(formula = dist ~ speed, data = cars)
```

# The lm object

The `lm()` function creates an object of class `"lm"`. Many R functions have convenient methods for this object that are useful for understanding and using the output of a regression model. 

```{r}
# sample 1,000 observations and set ordinal factors to nominal
set.seed(20200622)

diamonds <- diamonds %>%
  sample_n(1000) %>%
  mutate(across(where(is.factor), .fns = as.character))

```

```{r}
# estimate a multiple linear regression model
diamonds_model1 <- lm(formula = price ~ carat + cut, data = diamonds)

class(diamonds_model1)

```

## Summary

`summary()` returns a regression table with the call, a five number summary of the residuals, coefficient estimates, standard errors, t statistics, p-values, the residual standard error, $R^2$, adjusted $R ^ 2$, the F-statistic, and the p-value for the F-statistic. 

```{r}
summary(diamonds_model1)
```

## Coefficients

`coef()` can be used to select just the coefficients:

```{r}
coef(diamonds_model1)
```

## Residuals

`resid()` can used to select just a vector of the residuals. 

```{r}
resid(diamonds_model1)[1:10]
```

## Diagnostic plots

`plot()` will return four plots with regression diagnostics. 

* **(1) Residual plot:** This demonstrates if the residuals have non-linear patterns or non-constant variance. 
* **(2) Normal QQ plot:** This demonstrates if the residuals are normally distributed. 
* **(3) Scale-Location plot:** This also demonstrates if the residuals have non-constant variance.
* **(4): Residuals vs. leverage plot** This demonstrates cases that may be influential.

```{r}
plot(diamonds_model1)
```

# `library(modelr)`

`library(modelr)` has many useful functions for modeling. It works with more types of models than just linear models from `lm()`. 

## `add_predictions()`

`add_predictions()` adds predictions to a data set using an estimated model object. 

```{r}
add_predictions(data = diamonds, model = diamonds_model1)
```

## `add_residuals()`

`add_residuals()` adds residuals to a data set using an estimated model object. 

```{r}
add_residuals(data = diamonds, model = diamonds_model1)
```

## `data_grid()`

`data_grid()` creates an evenly-spaced grid of points using the range of observed predictors in a data set. This is useful for visualization and is really, really useful for understanding generalized linear models. 

```{r}
data_grid(data = diamonds, carat, cut) %>%
  add_predictions(diamonds_model1)
```

```{r echo = FALSE}
cut_levels <- c("Fair", "Good",  "Very Good", "Ideal", "Premium")

data_grid(data = diamonds, carat, cut) %>%
  add_predictions(diamonds_model1) %>%
  mutate(cut = factor(cut, levels = cut_levels)) %>%
  ggplot(aes(carat, pred, color = cut)) +
  geom_line(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 3),
                     expand = c(0, 0)) +
  scale_y_continuous(limits = c(-5000, 20000),
                     expand = c(0, 0),
                     labels = scales::dollar) +
  scatter_grid() +
  labs(title = "data_grid is useful for interpreting the regression line")
```

# `library(broom)`

`library(broom)` contains three helpful functions for tidying the output of estimated models. It has methods for many types of models. We will demonstrate applications with `lm()`. 

Estimated regression functions have diagnostics at the model level, at the coefficient level, and at the observation level. 

## `glance()`

`glance()` returns model-level diagnostics like $R^2$ and $\hat{\sigma}$. 

```{r}
glance(diamonds_model1)
```

## `tidy()`

`tidy()` returns coefficient-level diagnostics like standard errors and p-values.

```{r}
tidy(diamonds_model1)
```

## `augment()`

`augment()` returns observation-level diagnostics like residuals and hat values. 

```{r}
augment(diamonds_model1)
```

# Data viz

`library(broom)` is incredibly helpful for data visualization. 

## Coefficient plot

Here's a simple plot of estimated OLS coefficients and their 95% confidence intervals. 

```{r}
tidy(diamonds_model1, 
     conf.int = TRUE,
     conf.level = 0.95) %>%
  ggplot(aes(x = estimate, 
             y = term,
             xmin = conf.low,
             xmax = conf.high)) +
  geom_pointrange() +
  geom_vline(xintercept = 0) +
  scale_x_continuous(limits = c(-10000, 10000),
                     labels = scales::dollar) +
  scatter_grid() +
  labs()
```

## Residual plot

Here's an even simpler residual plot. The non-constant errors are a big issue and our specification is clearly inadequate!

```{r}
augment(diamonds_model1) %>%
  ggplot(aes(.fitted, .resid)) +
  geom_point(alpha = 0.2) +
  scatter_grid()
```

## Many models

```{r}
# estimate a linear model for each of seven colors
many_models <- diamonds %>%
  split(diamonds$color) %>%
  map(~lm(formula = price ~ carat + cut, data = .))

# extract model diagnostics from each model
many_models_results <- bind_cols(
  color = names(many_models), 
  map_df(many_models, glance)
)

# plot
many_models_results %>%
  ggplot(aes(color, r.squared)) +
  geom_col() +
  scale_y_continuous(expand = c(0, 0),
                     limits = c(0, 1)) +
  labs(title = "R-Squared for linear models estimated on subsets by color") +
  remove_ticks()
```

[Hadley Wickham gave a great talk about this to the The Edinburgh R User Group.](https://www.youtube.com/watch?v=rz3_FDVt9eg)

### `r kableExtra::text_spec("Exercise 1", color = "#1696d2")`

<font color="#55b748">Step 1:</font> Pick a data set. It could be your own or it could be from `data()`. Basic options include `msleep` and `mpg`. 

<font color="#55b748">Step 2:</font> Estimate a simple or multiple linear regression with `lm()`. The model does not need to be great. 

<font color="#55b748">Step 3:</font> Use `library(broom)` and create a visualization that includes the coefficients.

<font color="#55b748">Step 4:</font> Use `library(broom)` and create a visualization that includes data at the observation level (i.e. residuals, predicted values, etc.).
